# -*- coding: utf-8 -*-
"""Autism prediction using ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HVizun3T8bo71TVpFc5bl5WZHKbW0P_g

Importing the dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
import pickle

"""**2.Data loading and understanding**"""

#read the csv data to a pandas data frame
df=pd.read_csv("C:/Users/Jothsna naidu/Downloads/train.csv")

"""Initial Inspection

"""

df.shape
#gives no.of rows and cols

df.head()

df.tail()
#last 5 rows

#display all columns of data frame
pd.set_option('display.max_columns',None)

df.info()

#convert age column data type to integer
df["age"] = df["age"].astype(int)

df.head(2)

#trying to find unique values and make sure they are categorical
for col in df.columns:
  numerical_features = ["ID","age","result"]
  #ID column is skipped
  if col not in numerical_features:
    print(col,df[col].unique())
    print("-"*50)

#dropping ID & age_desc
df=df.drop(columns=["ID","age_desc"])

df.columns

df["contry_of_res"].unique()

#defining mapping dictionary for country names
mapping={
    " Viet Nam":"Vietnam",
    "AmericanSamoa":"United States",
    "Hong Kong":"China"
}
#replace value in the country column
df["contry_of_res"] = df["contry_of_res"].replace(mapping)

df["contry_of_res"].unique()

#target class Distribution
df["Class/ASD"].value_counts()

"""**Insights**

1.missing values in ethnicity & relation

2.age_desc column has only 1 unique value.so it is removed as it is not important.

3.fixed the country names.

4.identified class imbalance in the target column

**3.Exploratory Data Analysis (EDA)**
"""

df.shape

df.columns

df.head(2)

#gives mean,median---so on
df.describe()

"""**UniVariate Analysis**

Numerical Columns:-
* age
* result
"""

#to understand distribution->histogram plot
#box plot to detect outliers
#set the desired theme
sns.set_theme(style="darkgrid")

"""Distribution Plots"""

#histogram for age
sns.histplot(df["age"],kde=True)
plt.title("Distribution of Age")

age_mean=df["age"].mean()
age_median=df["age"].median()
print("Mean:",age_mean)
print("Median:",age_median)

#add vertical lines for mean and median
plt.axvline(age_mean,color="red",linestyle="--",label="Mean")
plt.axvline(age_median,color="green",linestyle="-",label="Median")
plt.legend()
plt.show()

"""in case of regression models the models expect the distribution to be linear or normal"""

#histogram for result
sns.histplot(df["result"],kde=True)
plt.title("Distribution of result")

result_mean=df["result"].mean()
result_median=df["result"].median()
print("Mean:",result_mean)
print("Median:",result_median)

#add vertical lines for mean and median
plt.axvline(result_mean,color="red",linestyle="--",label="Mean")
plt.axvline(result_median,color="green",linestyle="-",label="Median")
plt.legend()
plt.show()

"""Box plots for identifying outliers in the numerical columns"""

#box plot
sns.boxplot(x=df["age"])
plt.title("Box plot for Age")
plt.xlabel("Age")
plt.show()

#box plot
sns.boxplot(x=df["result"])
plt.title("Box plot for result")
plt.xlabel("result")
plt.show()

#count the outliers using IQR method
Q1=df["age"].quantile(0.25)
Q3=df["age"].quantile(0.75)
IQR=Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
age_outliers=df[(df["age"]< lower_bound) | (df["age"] > upper_bound)]

len(age_outliers)

#count the outliers using IQR method
Q1=df["result"].quantile(0.25)
Q3=df["result"].quantile(0.75)
IQR=Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
result_outliers=df[(df["result"]< lower_bound) | (df["result"] > upper_bound)]

len(result_outliers)

"""Univariate analysis of Categorical Columns

"""

df.columns

categorical_columns=['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score',
       'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score',  'gender',
       'ethnicity', 'jaundice', 'austim', 'contry_of_res', 'used_app_before',
        'relation']
for col in categorical_columns:
  sns.countplot(x=df[col])
  plt.title(f"Count Plot for {col}")
  plt.xlabel(col)
  plt.ylabel("Count")
  plt.show()

sns.countplot(x=df["Class/ASD"])
plt.title("Count Plot for Class/ASD")
plt.xlabel("Class/ASD")
plt.ylabel("Count")
plt.show()

df["Class/ASD"].value_counts()

"""handle missing values in ethincity and relation in certain column"""

df["ethnicity"] = df["ethnicity"].replace({"?":"Others","others":"Others"})

df["ethnicity"].unique()

df["relation"].unique()

df["relation"] = df["relation"].replace(
    {"?":"others",
     "Relative":"Others",
     "Parent":"Others",
     "Health care Professional":"Others"}
)

df["relation"].unique()

df.head()

"""**label Encoding**"""

#apply label encoding for all columns instead of individual
#identify string categorical columns
#get object typed columns and apply label encoding
object_columns = df.select_dtypes(include=["object"]).columns

print(object_columns)

#initialise a dictionary to store the encoders
encoders={}
#apply label encoding and store the encoders
for column in object_columns:
  label_encoder = LabelEncoder()
  df[column] = label_encoder.fit_transform(df[column])
  encoders[column]=label_encoder #saving the encoder for this column

#save the encoders as a pickle file
with open("encoders.pkl","wb") as f:
  pickle.dump(encoders,f)

encoders

df.head()
#categorical stream values into numericals

"""Bivariate Analysis"""

#correlation matrix
plt.figure(figsize=(16,16))
sns.heatmap(df.corr(),annot=True,cmap="coolwarm",fmt=".2f")
plt.title("Correlation heatmap")
plt.show()

"""**Insights from EDA**
- there are few outliers in the numerical columns(age,results)
- there is class imbalance in the target column
- there is class imbalance in the categorical features
- We dont have any highly correlated columns
- performed label encoding and saved the encoders

**4.Data PreProcessing**

Handling teh outliers
"""

#function to replace the outliers with median
def replace_outliers_with_median(df,column):
  Q1=df["age"].quantile(0.25)
  Q3=df["age"].quantile(0.75)
  IQR=Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  median = df[column].median()

  #replace outliers with median value
  df[column] = df[column].apply(lambda x : median if x < lower_bound or x > upper_bound else x)
  return df

from os import replace
#replace outliers in the age column
df = replace_outliers_with_median(df,"age")

#replace outliers in the result column
df = replace_outliers_with_median(df,"result")

df.head()

df.shape

"""**Train Test Split**"""

X = df.drop(columns=["Class/ASD"])
y = df["Class/ASD"]

print(X)

print(y)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

print(y_train.shape)
print(y_test.shape)

y_train.value_counts()

y_test.value_counts()

"""**SMOTE(Synthetic Minority OverSampling Technique)**"""

smote = SMOTE(random_state=42)

X_train_smote,y_train_smote = smote.fit_resample(X_train,y_train)

print(y_train_smote.shape)

print(y_train_smote.value_counts())

"""**Model Training**"""

#list of classifiers
models = {
    "Decision Tree":DecisionTreeClassifier(random_state=42),
    "Random Forest":RandomForestClassifier(random_state=42),
    "XGBoost":XGBClassifier(random_state=42)
}

#dictionary to store th cross validation results
cv_scores = {}
#perform 5 -fold cross validation for each model
for model_name,model in models.items():
  print(f"Training {model_name} with default parameters...")
  scores=cross_val_score(model,X_train_smote,y_train_smote,cv=5,scoring="accuracy")
  cv_scores[model_name] = scores
  print(f"{model_name} Cross-validation Accuracy:{np.mean(scores):.2f}")
  print("-"*50)

cv_scores

"""**6.Model Selection and HyperParameter Tuning**"""

#Initialising models
decision_tree=DecisionTreeClassifier(random_state=42)
random_forest=RandomForestClassifier(random_state=42)
xgboost = XGBClassifier(random_state=42)

#Hyperparameter grids for RandomisedSearchCV
param_grid_dt = {
    "criterion":["gini","entropy"],
    "max_depth":[None,5,10,20,30,50,70],
    "min_samples_split":[2,5,10],
    "min_samples_leaf":[1,2,4]
}
param_grid_rf={
    "n_estimators":[50,100,200,500],
    "max_depth":[None,10,20,30],
    "min_samples_split":[2,5,10],
    "min_samples_leaf":[1,2,4],
    "bootstrap":[True,False]
}

param_grid_xgb={
    "n_estimators":[50,100,200,500],
    "max_depth":[3,5,7,10],
    "learning_rate":[0.01,0.1,0.2,0.3],
    "subsample":[0.5,0.7,1.0],
    "colsample_bytree":[0.5,0.7,1.0]
}

#hyper parameter tuning for 3 tree based models
#the below steps can be automated by for loop or by using pipe line
random_search_dt = RandomizedSearchCV(estimator=decision_tree,param_distributions=param_grid_dt,n_iter=20,cv=5,scoring="accuracy",random_state=42)
random_search_rf = RandomizedSearchCV(estimator=random_forest,param_distributions=param_grid_rf,n_iter=20,cv=5,scoring="accuracy",random_state=42)
random_search_xgb = RandomizedSearchCV(estimator=xgboost,param_distributions=param_grid_xgb,n_iter=20,cv=5,scoring="accuracy",random_state=42)

#fit models
random_search_dt.fit(X_train_smote,y_train_smote)
random_search_rf.fit(X_train_smote,y_train_smote)
random_search_xgb.fit(X_train_smote,y_train_smote)

print(random_search_dt.best_estimator_)
print(random_search_dt.best_score_)

print(random_search_rf.best_estimator_)
print(random_search_rf.best_score_)

print(random_search_xgb.best_estimator_)
print(random_search_xgb.best_score_)

#Get the model with best score
best_model=None
best_score=0
if random_search_dt.best_score_ > best_score:
  best_model=random_search_dt.best_estimator_
  best_score=random_search_dt.best_score_

if random_search_rf.best_score_> best_score:
  best_model=random_search_rf.best_estimator_
  best_score=random_search_rf.best_score_

if random_search_xgb.best_score_ > best_score:
  best_model=random_search_xgb.best_estimator_
  best_score=random_search_xgb.best_score_

print(f"Best Model:{best_model}")
print(f"Best Cross Validation Accuracy Score:{best_score:.2f}")

#save the best model
with open("best_model.pkl","wb") as f:
  pickle.dump(best_model,f)

# Save the feature order used during training
feature_order = list(X_train.columns)
with open("feature_order.pkl", "wb") as f:
    pickle.dump(feature_order, f)

"""**Evaluation**"""

#evaluate on test data
y_test_pred = best_model.predict(X_test)
print("Accuracy score:\n",accuracy_score(y_test,y_test_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_test_pred))
print("Classification report:\n",classification_report(y_test,y_test_pred))

"""To do:
- 1.Build a predictive system with encoders and model file
-2.See if you could improve the perfomance

Saving the trained model
"""

import pickle

with open("best_model.pkl", "wb") as f:
    pickle.dump(best_model, f)

with open("feature_order.pkl", "wb") as f:
    pickle.dump(feature_order, f)

with open("encoders.pkl", "wb") as f:
    pickle.dump(encoders, f)
